{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling of Case Law on Unemployment using LDA\n",
    "조희진, 맹주리, 우정연\n",
    "* Background: Rise of interest in AI in the legal field\n",
    "* Objective: To identify unfair dismissal case law through Latend Diriclet Allocation (LDA) analysis \n",
    "* Process: data preprocessing (drop unwanted columns, define stop words) -> compare stemming methods -> perform LDA\n",
    "* Conclusion\n",
    "    * Divided into 10 topics\n",
    "    * Provided new insights to understanding the grouped cases\n",
    "* Takeaways\n",
    "    * Tokenizing needs to be more considerate of the legal characteristics\n",
    "    * May need to perform more sophisticated preprocessing so that unimportant information is not included in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>판례내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【원고, 상고인】 원고 1 외 1인 (소송대리인 법무법인 여는 담당변호사 정기호 외...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>【원고, 상고인】 원고 (소송대리인 법무법인 어울림 담당변호사 구은미 외 2인)【피...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>【원고, 상고인】 원고 1 외 1인 (소송대리인 법무법인 여는 담당변호사 탁선호 외...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【원고, 피상고인 겸 상고인】 원고 (소송대리인 법무법인 세진 담당변호사 권구철 외...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【원고】 원고 (소송대리인 법무법인 서린 담당변호사 조석영 외 1인)【피고】 수원지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>【원고, 상고인 겸 피상고인】   왕호식 외 31인【피고, 상고인 겸 피상고인】  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>【원고, 상고인】 【피고, 피상고인】   유건묵【원심판결】 \\n제1심 서울민사지법,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>【원고, 피상고인】   유건목 외 1인【피고, 상고인】   한국전력주식회사【원심판결...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>【원고, 상고인】 【피고, 피상고인】 【원심판결】 \\n제1심 서울지방, 제2심 \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>【원고, 피상고인】   선정당시자 \\n          \\n          \\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>735 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  판례내용\n",
       "0    【원고, 상고인】 원고 1 외 1인 (소송대리인 법무법인 여는 담당변호사 정기호 외...\n",
       "1    【원고, 상고인】 원고 (소송대리인 법무법인 어울림 담당변호사 구은미 외 2인)【피...\n",
       "2    【원고, 상고인】 원고 1 외 1인 (소송대리인 법무법인 여는 담당변호사 탁선호 외...\n",
       "3    【원고, 피상고인 겸 상고인】 원고 (소송대리인 법무법인 세진 담당변호사 권구철 외...\n",
       "4    【원고】 원고 (소송대리인 법무법인 서린 담당변호사 조석영 외 1인)【피고】 수원지...\n",
       "..                                                 ...\n",
       "730  【원고, 상고인 겸 피상고인】   왕호식 외 31인【피고, 상고인 겸 피상고인】  ...\n",
       "731  【원고, 상고인】 【피고, 피상고인】   유건묵【원심판결】 \\n제1심 서울민사지법,...\n",
       "732  【원고, 피상고인】   유건목 외 1인【피고, 상고인】   한국전력주식회사【원심판결...\n",
       "733  【원고, 상고인】 【피고, 피상고인】 【원심판결】 \\n제1심 서울지방, 제2심 \\n...\n",
       "734  【원고, 피상고인】   선정당시자 \\n          \\n          \\n ...\n",
       "\n",
       "[735 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case law data from Korea, preprocessed\n",
    "data=pd.read_csv(\"판례본문_해고_수정.csv\")\n",
    "data=data.drop(columns=['판례정보일련번호', '사건번호', '선고일자', '법원종류코드', '판시사항', '판결요지', '참조조문', '참조판례', '사건명'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"판례내용.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared various tag classes (wrappers) from the KoNLPy package for text tokenizing.\\\n",
    "The tokenizing processes for Kkma and Komoran are not included in this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maeng\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "def split_sentences(text):\n",
    "    \n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def get_pos(analyzer, text):\n",
    "    \n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)                \n",
    "        morph_anals.append(morph_anal)\n",
    "        \n",
    "    return morph_anals\n",
    "\n",
    "def read_text(input_file_name):        \n",
    "    \n",
    "    key_names = ['판례내용']\n",
    "    data = []                        \n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\", newline=\"\") as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        for row_num, row in enumerate(reader):\n",
    "            if row_num == 0:\n",
    "                continue\n",
    "\n",
    "            reviews = {}\n",
    "\n",
    "            for key_name, val in zip(key_names, row):\n",
    "                reviews[key_name] = val\n",
    "\n",
    "            data.append(reviews)\n",
    "\n",
    "    return data\n",
    "\n",
    "def pos_review(data):  \n",
    "    \n",
    "    data_pos = []\n",
    "    twitter = Twitter()\n",
    "    \n",
    "    for reviews in data:\n",
    "        body = reviews[\"판례내용\"]                     \n",
    "        review_pos = get_pos(twitter, body)        \n",
    "        reviews[\"판례내용_pos\"] = review_pos              \n",
    "        data_pos.append(reviews)\n",
    "\n",
    "    return data_pos\n",
    "\n",
    "def write_pos_review(output_file_name, data_pos):       \n",
    "    \n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        for review_pos in data_pos:\n",
    "            review_str = ujson.dumps(review_pos, ensure_ascii=False)\n",
    "            print(review_str, file=output_file)\n",
    "            \n",
    "\n",
    "def main(): \n",
    "    \n",
    "    input_file_name = r\"판례내용.csv\"\n",
    "    output_file_name = r\"twitter_pos_판례내용.txt\"\n",
    "    \n",
    "    data = read_text(input_file_name)                                         \n",
    "    data_pos = pos_review(data)                                           \n",
    "    write_pos_review(output_file_name, data_pos)            \n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(input_file_name):\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            json_obj = ujson.loads(line)\n",
    "            text_pos = json_obj[POS_KEY]\n",
    "            \n",
    "            words = []\n",
    "\n",
    "            stop_words = ['징계','원고', '피고', '상고인', '피상고인', '다음', '부터', '참가인', '소외', '따르','등','회사', '상고',\n",
    "                          '위원회',\n",
    "                          '해고','근로자', '이','판결','하','있','것','위','선고','제','대하','원심','없','같','수','의하','관하',\n",
    "                          '항','받','이', '있', '하', '것', '들', '그', '되', '수',\n",
    "                          '이', '보', '않', '없', '나', '사람', '주', '아니',\n",
    "                          '등', '같', '우리', '때', '년', '가', '한', '지', '대하',\n",
    "                          '오', '말', '일', '그렇', '위하', '및',\n",
    "                          '원고', '상고인', '피고', '피상고인', '제', '위', '항', '바', '관', '점',\n",
    "                          '불', '중', '볼', '후', '함', '명', '의', '각', '호', '정', '자', '경', '고', '것이므', '해', '금',\n",
    "                          '부터', '다음', '뿐','조','증인','증', '재심', '호증','제법','단체','보조참가인','항소','위원', '공사','소론',\n",
    "                          '심','임자','소정','아니하','을','주장','경우','사실','당원','고인','피상','서울','대리인','항소인','문',\n",
    "                          '고등','변호사','유','주문','원','위원회',\n",
    "                          '대법원','외','재판장','이유','법관','기각','대법관','김','가집행','부담','비용','판사','소론','패소자',\n",
    "                          '박','철','주심','한편','위원','외인','소',\n",
    "                          '측','해당','참조','카','다','누','부장','정자','동인','화학','합계','지법','급','믿','법','달',\n",
    "                          '동부','구','금','소외','을','심','보조참가','원심']\n",
    "            \n",
    "            for sent_pos in text_pos:\n",
    "                for word, pos in sent_pos:\n",
    "                    if pos not in FEATURE_POS:\n",
    "                        continue\n",
    "\n",
    "                    words.append(word)\n",
    "                    \n",
    "            words_new = []\n",
    "            for word in words:\n",
    "                if word not in stop_words:\n",
    "                    words_new.append(word)\n",
    "\n",
    "            documents.append(words_new)\n",
    "         \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735\n"
     ]
    }
   ],
   "source": [
    "FEATURE_POS = [\"Noun\"]\n",
    "POS_KEY = \"판례내용_pos\"\n",
    "\n",
    "input_file_name = r\"twitter_pos_판례내용.txt\"\n",
    "documents = read_documents(input_file_name)\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "n_items = len(dictionary)\n",
    "doc = [dictionary.doc2bow(text) for text in documents] \n",
    "tfidf = models.TfidfModel(doc)\n",
    "corpus = tfidf[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "n_items = len(dictionary)\n",
    "doc = [dictionary.doc2bow(text) for text in documents] \n",
    "tfidf = models.TfidfModel(doc)\n",
    "corpus = tfidf[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "lda_model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary,\n",
    "                                     passes=2, iterations=10, chunksize=350, eval_every=None, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "pyLDAvis.save_html(vis_data, \"tfidf_twitter_10.html\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9250\n"
     ]
    }
   ],
   "source": [
    "FEATURE_POS = [\"NNG\",\"NNP\",\"NNB\", \"VV\", \"VA\", \"VXV\", \"VXA\", \"VCP\", \"VCN\",\"MM\"]\n",
    "POS_KEY = \"판례_pos\"\n",
    "\n",
    "input_file_name = r\"pos_kkma.txt\"\n",
    "documents = read_documents(input_file_name)\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "n_items = len(dictionary)\n",
    "doc = [dictionary.doc2bow(text) for text in documents]\n",
    "tfidf = models.TfidfModel(doc)\n",
    "corpus = tfidf[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "lda_model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary,\n",
    "                                     passes=2, iterations=10, chunksize=350, eval_every=None, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.013*\"의원\" + 0.009*\"고함\" + 0.008*\"권고\" + 0.007*\"목\" + 0.006*\"당부\" + 0.006*\"가운데\" '\n",
      "  '+ 0.005*\"제재\" + 0.005*\"이름\" + 0.005*\"끝\" + 0.004*\"불성실\"'),\n",
      " (1,\n",
      "  '0.006*\"불참\" + 0.004*\"공무원\" + 0.003*\"부인\" + 0.002*\"공소\" + 0.001*\"명예퇴직\" + '\n",
      "  '0.001*\"울산\" + 0.001*\"중지\" + 0.000*\"국가\" + 0.000*\"정당법\" + 0.000*\"건설\"'),\n",
      " (2,\n",
      "  '0.019*\"논지\" + 0.014*\"사유\" + 0.013*\"판단\" + 0.013*\"처분\" + 0.011*\"위법\" + 0.011*\"인정\" '\n",
      "  '+ 0.009*\"진술\" + 0.008*\"법리\" + 0.008*\"오해\" + 0.008*\"기록\"'),\n",
      " (3,\n",
      "  '0.012*\"해제\" + 0.001*\"비리\" + 0.000*\"감경\" + 0.000*\"직위\" + 0.000*\"라\" + 0.000*\"선우\" '\n",
      "  '+ 0.000*\"본부\" + 0.000*\"평점\" + 0.000*\"최장\" + 0.000*\"순응\"'),\n",
      " (4,\n",
      "  '0.037*\"일치\" + 0.036*\"관여\" + 0.035*\"의견\" + 0.016*\"우동\" + 0.015*\"윤영\" + '\n",
      "  '0.013*\"김용준\" + 0.012*\"김상원\" + 0.010*\"이회\" + 0.010*\"윤\" + 0.009*\"배석\"'),\n",
      " (5,\n",
      "  '0.013*\"위의\" + 0.008*\"임시\" + 0.008*\"사립학교법\" + 0.008*\"유예\" + 0.008*\"상해\" + '\n",
      "  '0.006*\"조처\" + 0.006*\"총회\" + 0.006*\"가담\" + 0.006*\"재판\" + 0.006*\"징역\"'),\n",
      " (6,\n",
      "  '0.017*\"배포\" + 0.008*\"약식\" + 0.007*\"명예\" + 0.006*\"인격\" + 0.006*\"위조\" + 0.006*\"대리\" '\n",
      "  '+ 0.005*\"덕\" + 0.004*\"흔적\" + 0.004*\"훼손\" + 0.003*\"게시\"'),\n",
      " (7,\n",
      "  '0.025*\"적용\" + 0.020*\"학력\" + 0.018*\"이력서\" + 0.014*\"주한\" + 0.013*\"증언\" + '\n",
      "  '0.011*\"졸업\" + 0.008*\"생기\" + 0.006*\"대우\" + 0.006*\"과\" + 0.005*\"생산직\"'),\n",
      " (8,\n",
      "  '0.010*\"지급\" + 0.008*\"청구\" + 0.008*\"수당\" + 0.008*\"근로\" + 0.008*\"소송\" + 0.008*\"민사\" '\n",
      "  '+ 0.006*\"무효\" + 0.006*\"소송법\" + 0.006*\"규정\" + 0.006*\"인정\"'),\n",
      " (9,\n",
      "  '0.025*\"기준법\" + 0.018*\"근로\" + 0.013*\"규정\" + 0.012*\"노동조합\" + 0.011*\"협약\" + '\n",
      "  '0.010*\"선정\" + 0.010*\"취업\" + 0.010*\"규칙\" + 0.010*\"정리\" + 0.009*\"조합법\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "pyLDAvis.save_html(vis_data, \"tfidf_kkma_10.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735\n"
     ]
    }
   ],
   "source": [
    "FEATURE_POS = [\"NNG\", \"NNP\", \"VV\", \"VA\",\"VX\",\"MM\",\"NP\",\"NR\"]\n",
    "POS_KEY = \"content_pos\"\n",
    "\n",
    "input_file_name = r\"pos_komoran.txt\"\n",
    "documents = read_documents(input_file_name)\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "n_items = len(dictionary)\n",
    "doc = [dictionary.doc2bow(text) for text in documents]\n",
    "tfidf = models.TfidfModel(doc)\n",
    "corpus = tfidf[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "lda_model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary,\n",
    "                                     passes=2, iterations=10, chunksize=350, eval_every=None, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.001*\"동해\" + 0.000*\"체육\" + 0.000*\"선원\" + 0.000*\"교정\" + 0.000*\"시력\" + '\n",
      "  '0.000*\"지도자\" + 0.000*\"갱신\" + 0.000*\"좌\" + 0.000*\"계약\" + 0.000*\"기간\"'),\n",
      " (1,\n",
      "  '0.006*\"협회\" + 0.005*\"보건\" + 0.004*\"반려\" + 0.002*\"중학교\" + 0.002*\"가처분\" + '\n",
      "  '0.002*\"병원\" + 0.002*\"주임\" + 0.002*\"관광버스\" + 0.001*\"탈퇴\" + 0.001*\"배포\"'),\n",
      " (2,\n",
      "  '0.001*\"교회\" + 0.001*\"빌딩\" + 0.001*\"승계\" + 0.000*\"해산\" + 0.000*\"삼미\" + 0.000*\"호텔\" '\n",
      "  '+ 0.000*\"조교\" + 0.000*\"식품위생\" + 0.000*\"학사\" + 0.000*\"연구원\"'),\n",
      " (3,\n",
      "  '0.000*\"운항\" + 0.000*\"반출\" + 0.000*\"주민\" + 0.000*\"자원봉사\" + 0.000*\"원료\" + '\n",
      "  '0.000*\"덤프\" + 0.000*\"검문\" + 0.000*\"완성\" + 0.000*\"이현우\" + 0.000*\"기후\"'),\n",
      " (4,\n",
      "  '0.007*\"경력\" + 0.005*\"이력서\" + 0.005*\"사칭\" + 0.004*\"교통사고\" + 0.004*\"입사\" + '\n",
      "  '0.004*\"운전사\" + 0.004*\"사원\" + 0.003*\"운전\" + 0.003*\"사고\" + 0.003*\"학력\"'),\n",
      " (5,\n",
      "  '0.010*\"신청인\" + 0.001*\"승계\" + 0.001*\"양수\" + 0.001*\"양도\" + 0.001*\"귀국\" + '\n",
      "  '0.001*\"영업양도\" + 0.001*\"기숙사\" + 0.001*\"가처분\" + 0.001*\"전보발령\" + 0.001*\"감천\"'),\n",
      " (6,\n",
      "  '0.001*\"기대권\" + 0.000*\"갱신\" + 0.000*\"가공\" + 0.000*\"재계약\" + 0.000*\"전적\" + '\n",
      "  '0.000*\"산림조합\" + 0.000*\"단속\" + 0.000*\"연고지\" + 0.000*\"조합\" + 0.000*\"파산\"'),\n",
      " (7,\n",
      "  '0.005*\"지급\" + 0.004*\"수당\" + 0.004*\"갑\" + 0.004*\"면직\" + 0.003*\"처분\" + 0.003*\"청구\" '\n",
      "  '+ 0.003*\"인사\" + 0.003*\"증언\" + 0.002*\"취업규칙\" + 0.002*\"퇴직금\"'),\n",
      " (8,\n",
      "  '0.004*\"구미\" + 0.001*\"외출\" + 0.001*\"협회\" + 0.001*\"시말서\" + 0.001*\"이의신청\" + '\n",
      "  '0.001*\"승무원\" + 0.000*\"후원금\" + 0.000*\"생년월일\" + 0.000*\"전적\" + 0.000*\"사무규정\"'),\n",
      " (9,\n",
      "  '0.001*\"총회\" + 0.001*\"직위해제\" + 0.001*\"특례\" + 0.001*\"기피\" + 0.001*\"동수\" + '\n",
      "  '0.001*\"변론기일\" + 0.000*\"직할시\" + 0.000*\"행정소송법\" + 0.000*\"중기\" + 0.000*\"변명\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected Komoran because the tokenizing results are more detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "pyLDAvis.save_html(vis_data, \"tfidf_komoran_10.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
